{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b905323c",
   "metadata": {},
   "source": [
    "# 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c38354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0976be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_logical_device_configuration(gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83a6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 14:42:58.626935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-15 14:42:59.316257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:17:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from pprint import pprint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13729a",
   "metadata": {},
   "source": [
    "# 1. Data and Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f7250",
   "metadata": {},
   "source": [
    "## 1-1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cc263",
   "metadata": {},
   "source": [
    "### FinPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e416b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fpath = './data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec79a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total data: 2264\n",
      "\n",
      "Data examples:\n",
      "['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral', \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .@positive\"]\n"
     ]
    }
   ],
   "source": [
    "with open(data_fpath, 'rb') as file:\n",
    "    data = file.read()\n",
    "data = data.decode('utf-8', 'ignore')\n",
    "data = data.split('\\r\\n')[:-1]\n",
    "print('Number of total data: %d\\n' % len(data))\n",
    "print('Data examples:')\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1f71c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title examples:\n",
      "['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\"]\n",
      "\n",
      "Label examples\n",
      "['neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "titles = [line.split('@')[0] for line in data]\n",
    "labels = [line.split('@')[1] for line in data]\n",
    "print('Title examples:')\n",
    "print(titles[:2])\n",
    "print('\\nLabel examples')\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf18c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sech/anaconda3/envs/cl2022/lib/python3.8/site-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "label_list = ['positive', 'negative', 'neutral']\n",
    "Y = tf.keras.layers.StringLookup(vocabulary=label_list,\n",
    "                                 num_oov_indices=0, output_mode='one_hot')(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905f200",
   "metadata": {},
   "source": [
    "### SemEval 2017 Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e5fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fpath = './data/Headline_Trainingdata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdcd21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_fpath, 'r', encoding='utf-8') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# print('Number of total data: %d\\n' % len(data))\n",
    "# print('Data examples:')\n",
    "# pprint(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d97e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = []\n",
    "# companies = []\n",
    "# titles = []\n",
    "# sentiments = []\n",
    "# for i in range(len(data)):\n",
    "#     ids.append(data[i]['id'])\n",
    "#     companies.append(data[i]['company'])\n",
    "#     titles.append(data[i]['title'])\n",
    "#     sentiments.append(data[i]['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e587a07",
   "metadata": {},
   "source": [
    "## 1-2. FinBERT Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2241cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "config = AutoConfig.from_pretrained('ProsusAI/finbert',\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cefc87",
   "metadata": {},
   "source": [
    "## 1-3. Vectorization to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83fa9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(titles, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9fb82f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 2s, sys: 13min 1s, total: 41min 3s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a301cb",
   "metadata": {},
   "source": [
    "# 2. Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "492efc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee415f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>Operating result for the 12-month period decre...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>HELSINKI Thomson Financial - Shares in Cargote...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title     label\n",
       "0     According to Gran , the company has no plans t...   neutral\n",
       "1     For the last quarter of 2010 , Componenta 's n...  positive\n",
       "2     In the third quarter of 2010 , net sales incre...  positive\n",
       "3     Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
       "4     Operating profit totalled EUR 21.1 mn , up fro...  positive\n",
       "...                                                 ...       ...\n",
       "2259  Operating result for the 12-month period decre...  negative\n",
       "2260  HELSINKI Thomson Financial - Shares in Cargote...  negative\n",
       "2261  LONDON MarketWatch -- Share prices ended lower...  negative\n",
       "2262  Operating profit fell to EUR 35.4 mn from EUR ...  negative\n",
       "2263  Sales in Finland decreased by 10.5 % in Januar...  negative\n",
       "\n",
       "[2264 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'title': titles, 'label': labels})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58430b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "prob_idx = df.groupby('label').sample(n_sample, random_state=0).index\n",
    "prob_idx_neg = prob_idx[:n_sample]\n",
    "prob_idx_neu = prob_idx[n_sample:n_sample*2]\n",
    "prob_idx_pos = prob_idx[-n_sample:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9030057",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.math.argmax(tf.convert_to_tensor(outputs.logits), axis=1)\n",
    "Y_true = tf.math.argmax(Y, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad21474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by labels:\n",
      "Positive\t| Negative\t| Neutral\n",
      "98.0000\t\t| 99.0000\t| 96.0000\n"
     ]
    }
   ],
   "source": [
    "acc_pos = tf.reduce_sum(tf.cast((Y_pred == Y_true).numpy()[prob_idx_pos], tf.float32))\n",
    "acc_neg = tf.reduce_sum(tf.cast((Y_pred == Y_true).numpy()[prob_idx_neg], tf.float32))\n",
    "acc_neu = tf.reduce_sum(tf.cast((Y_pred == Y_true).numpy()[prob_idx_neu], tf.float32))\n",
    "print('Accuracy by labels:')\n",
    "print('Positive\\t| Negative\\t| Neutral')\n",
    "print('%.4f\\t\\t| %.4f\\t| %.4f' % (acc_pos, acc_neg, acc_neu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "636690ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sample - Error cases:\n",
      "345\n",
      "Unit costs for flight operations fell by 6.4 percent .\n",
      "predicted as: 1\n",
      "\n",
      "849\n",
      "The company reports a loss for the period of EUR 0.4 mn compared to a loss of EUR 1.9 mn in the corresponding period in 2005 .\n",
      "predicted as: 1\n",
      "\n",
      "--------------------\n",
      "Negative Sample - Error cases:\n",
      "371\n",
      "Finnish power supply solutions and systems provider Efore Oyj said its net loss widened to 3.2 mln euro $ 4.2 mln for the first quarter of fiscal 2006-2007 ending October 31 , 2007 from 900,000 euro $ 1.2 mln for the same period of fiscal 2005-06 .\n",
      "predicted as: 0\n",
      "\n",
      "--------------------\n",
      "Neutral Sample - Error cases:\n",
      "1108\n",
      "The sale of Savcor FACE to Cencorp will result in a profit or loss which can not yet be determined , owing to factors including the valuation of the consideration shares to be received and prevailing exchange rates .\n",
      "predicted as: 1\n",
      "\n",
      "1231\n",
      "Fortum expects its annual capital expenditure in the next four to five years to be within a range of EUR 0.8-1 .2 billion , as earlier announced .\n",
      "predicted as: 0\n",
      "\n",
      "1818\n",
      "The solution will be installed in the USA to support the North American operations of the customer .\n",
      "predicted as: 0\n",
      "\n",
      "1467\n",
      "The new location is n't the only change Wellmont has in store for its air transport service .\n",
      "predicted as: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Positive Sample - Error cases:')\n",
    "for idx in prob_idx_pos[(Y_pred != Y_true).numpy()[prob_idx_pos]]:\n",
    "    print(idx)\n",
    "    print(titles[idx])\n",
    "    print('predicted as: %d' % Y_pred[idx])\n",
    "    print()\n",
    "# [titles[idx] for idx in prob_idx[:n_sample][(Y_pred != Y_true)[-n_sample:]]]\n",
    "\n",
    "print('-'*20)\n",
    "print('Negative Sample - Error cases:')\n",
    "for idx in prob_idx_neg[(Y_pred != Y_true).numpy()[prob_idx_neg]]:\n",
    "    print(idx)\n",
    "    print(titles[idx])\n",
    "    print('predicted as: %d' % Y_pred[idx])\n",
    "    print()\n",
    "\n",
    "print('-'*20)\n",
    "print('Neutral Sample - Error cases:')\n",
    "for idx in prob_idx_neu[(Y_pred != Y_true).numpy()[prob_idx_neu]]:\n",
    "    print(idx)\n",
    "    print(titles[idx])\n",
    "    print('predicted as: %d' % Y_pred[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b029b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f6b7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_view_idx(idx):\n",
    "    input_text = titles[idx]\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    outputs = model(inputs)\n",
    "    attention = outputs[-1]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "    \n",
    "    model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebfd4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_view_idx(idx, fname):\n",
    "    input_text = titles[idx]\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    outputs = model(inputs)\n",
    "    attention = outputs[-1]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "    \n",
    "    html = model_view(attention, tokens, html_action='return')\n",
    "    \n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(html.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c9720e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_view_idx(1467, '1467.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1444ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "909cf269",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_ids = []\n",
    "\n",
    "for idx in prob_idx[(Y_pred != Y_true).numpy()[prob_idx]]:\n",
    "    results_ids.append(idx)\n",
    "    case_result = []\n",
    "    case_input = tokenizer(titles[idx], return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        case_output = model(**case_input)\n",
    "    for layer in case_output.attentions:\n",
    "        head_result = []\n",
    "        for head in layer[0]:\n",
    "            cls_attention = head[0]\n",
    "            token_id = np.argmax(cls_attention)\n",
    "            token = tokenizer.convert_ids_to_tokens(case_input.input_ids[0])[token_id]\n",
    "            head_result.append(token)\n",
    "        case_result.append(head_result)\n",
    "    results.append(case_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d87c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "814e328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'[SEP]': 73, '[CLS]': 37, 'loss': 10, '.': 8, 'compared': 7, 'the': 2, 'reports': 2, '##r': 1, 'for': 1, 'company': 1, 'corresponding': 1, 'mn': 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter([token for layer in case for token in layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4795999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case number 371: Finnish power supply solutions and systems provider Efore Oyj said its net loss widened to 3.2 mln euro $ 4.2 mln for the first quarter of fiscal 2006-2007 ending October 31 , 2007 from 900,000 euro $ 1.2 mln for the same period of fiscal 2005-06 .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\tthe\t[CLS]\t[CLS]\t##n\t[CLS]\t[CLS]\n",
      "Layer 1:\t,\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[SEP]\tfinnish\t[SEP]\t[SEP]\t[CLS]\tfinnish\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\tfinnish\tfinnish\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\tits\tsame\t.\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\t[CLS]\tsaid\tloss\tsaid\tfrom\t.\tloss\t.\tloss\t.\tloss\tloss\n",
      "Layer 11:\tto\tfrom\tfrom\tfrom\tloss\twidened\twidened\t.\tto\tsaid\tfrom\tfrom\n",
      "Counter({'[SEP]': 71, '[CLS]': 38, '.': 7, 'loss': 6, 'from': 6, 'finnish': 4, 'said': 3, 'to': 2, 'widened': 2, 'the': 1, '##n': 1, ',': 1, 'its': 1, 'same': 1})\n",
      "\n",
      "Case number 1108: The sale of Savcor FACE to Cencorp will result in a profit or loss which can not yet be determined , owing to factors including the valuation of the consideration shares to be received and prevailing exchange rates .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\tthe\t[CLS]\t[CLS]\tnot\t[CLS]\t[CLS]\n",
      "Layer 1:\tin\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[SEP]\tprevailing\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\t[CLS]\t##or\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\twill\t[SEP]\t.\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\twill\twill\t.\t[SEP]\towing\t.\t.\t.\t[SEP]\t.\twill\tloss\n",
      "Layer 11:\tprevailing\tloss\tto\twill\tloss\tloss\tprofit\t.\ta\ta\tloss\tloss\n",
      "Counter({'[SEP]': 75, '[CLS]': 37, '.': 10, 'loss': 6, 'will': 5, 'prevailing': 2, 'a': 2, 'the': 1, 'not': 1, 'in': 1, '##or': 1, 'owing': 1, 'to': 1, 'profit': 1})\n",
      "\n",
      "Case number 1231: Fortum expects its annual capital expenditure in the next four to five years to be within a range of EUR 0.8-1 .2 billion , as earlier announced .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\tthe\t[CLS]\t[CLS]\t,\t[CLS]\t[CLS]\n",
      "Layer 1:\tin\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[SEP]\t##um\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\t[CLS]\tcapital\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\tits\t[SEP]\t.\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\t[CLS]\tas\t.\t[SEP]\twithin\t.\teu\t.\t[SEP]\t.\t.\texpenditure\n",
      "Layer 11:\tto\trange\twithin\tto\twithin\tto\twithin\t.\tbe\tto\twithin\twithin\n",
      "Counter({'[SEP]': 77, '[CLS]': 37, '.': 9, 'within': 6, 'to': 4, 'the': 1, ',': 1, 'in': 1, '##um': 1, 'capital': 1, 'its': 1, 'as': 1, 'eu': 1, 'expenditure': 1, 'range': 1, 'be': 1})\n",
      "\n",
      "Case number 1818: The solution will be installed in the USA to support the North American operations of the customer .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\tthe\t[CLS]\t[CLS]\tof\t[CLS]\t[CLS]\n",
      "Layer 1:\t.\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[SEP]\tnorth\t[SEP]\t[SEP]\t[CLS]\tusa\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\t[CLS]\tinstalled\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\tthe\t[SEP]\t[SEP]\t[SEP]\t[SEP]\tsolution\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\twill\t.\tsolution\t[SEP]\tbe\t.\t.\t.\t[SEP]\t.\t.\tsupport\n",
      "Layer 11:\tinstalled\tinstalled\tsolution\tbe\tsupport\tsupport\tthe\t.\tto\tthe\tsupport\tsupport\n",
      "Counter({'[SEP]': 74, '[CLS]': 36, '.': 12, 'support': 5, 'the': 4, 'installed': 3, 'solution': 3, 'be': 2, 'of': 1, 'north': 1, 'usa': 1, 'will': 1, 'to': 1})\n",
      "\n",
      "Case number 1467: The new location is n't the only change Wellmont has in store for its air transport service .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\tthe\t[CLS]\t[CLS]\tt\t[CLS]\t[CLS]\n",
      "Layer 1:\tin\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[SEP]\tn\t[SEP]\t[SEP]\t[CLS]\tlocation\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\t[CLS]\tn\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\tonly\tis\tchange\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\tthe\tonly\t.\t[SEP]\tt\t.\t.\t.\t[SEP]\t.\t.\tchange\n",
      "Layer 11:\tchange\tchange\tchange\tonly\tfor\tchange\tchange\t.\tchange\tchange\tchange\tfor\n",
      "Counter({'[SEP]': 72, '[CLS]': 37, '.': 11, 'change': 10, 'only': 3, 'the': 2, 't': 2, 'n': 2, 'for': 2, 'in': 1, 'location': 1, 'is': 1})\n",
      "\n",
      "Case number 345: Unit costs for flight operations fell by 6.4 percent .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 1:\t.\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[CLS]\tflight\t[SEP]\t[SEP]\t[CLS]\tflight\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\tflight\tfell\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\tfell\t[SEP]\tby\t[SEP]\t[SEP]\tcosts\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\tfell\tfell\tpercent\t.\tfell\t.\t.\tflight\t[SEP]\t.\tfell\tfell\n",
      "Layer 11:\tfell\tfell\tfell\tfell\tfell\tfell\tfell\t.\tfell\tpercent\tby\tfell\n",
      "Counter({'[SEP]': 72, '[CLS]': 39, 'fell': 16, '.': 8, 'flight': 4, 'by': 2, 'percent': 2, 'costs': 1})\n",
      "\n",
      "Case number 849: The company reports a loss for the period of EUR 0.4 mn compared to a loss of EUR 1.9 mn in the corresponding period in 2005 .\n",
      "Layer 0:\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t.\tthe\t[CLS]\t[CLS]\t##r\t[CLS]\t[CLS]\n",
      "Layer 1:\tfor\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 2:\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\t[CLS]\n",
      "Layer 3:\t[SEP]\t[SEP]\treports\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[CLS]\t[CLS]\t[CLS]\t[SEP]\t[CLS]\n",
      "Layer 4:\t[SEP]\t[CLS]\tcompany\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[CLS]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 5:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 6:\tcompared\tcorresponding\t.\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 7:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t.\n",
      "Layer 8:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 9:\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\t[SEP]\n",
      "Layer 10:\tthe\tcompared\tloss\treports\tcompared\t.\tloss\t.\t[SEP]\t.\tloss\tloss\n",
      "Layer 11:\tcompared\tloss\tloss\tloss\tloss\tloss\tcompared\t.\tcompared\tmn\tcompared\tloss\n",
      "Counter({'[SEP]': 73, '[CLS]': 37, 'loss': 10, '.': 8, 'compared': 7, 'the': 2, 'reports': 2, '##r': 1, 'for': 1, 'company': 1, 'corresponding': 1, 'mn': 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for case, idx in zip(results, results_ids):\n",
    "    print('Case number %d: %s' % (idx, titles[idx]))\n",
    "    for i, layer in enumerate(case):\n",
    "        print('Layer %d:\\t%s' % (i, '\\t'.join(layer)))\n",
    "    print(Counter([token for layer in case for token in layer]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96511c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_prob = titles[:100]\n",
    "labels_prob = labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc405c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315eb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69139ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 12, 150), dtype=float32, numpy=\n",
       "array([[[1.0000001 , 1.        , 1.        , ..., 1.0000001 ,\n",
       "         1.        , 1.        ],\n",
       "        [0.99999994, 1.        , 1.        , ..., 1.        ,\n",
       "         1.        , 1.        ],\n",
       "        [1.        , 1.        , 0.9999999 , ..., 1.0000001 ,\n",
       "         1.0000001 , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.0000001 , 0.99999994, ..., 1.        ,\n",
       "         1.        , 0.99999994],\n",
       "        [0.9999999 , 0.99999994, 0.99999994, ..., 1.0000001 ,\n",
       "         1.        , 1.0000001 ],\n",
       "        [1.        , 1.        , 0.9999999 , ..., 0.9999998 ,\n",
       "         1.        , 1.0000002 ]],\n",
       "\n",
       "       [[1.        , 0.99999994, 0.99999994, ..., 1.        ,\n",
       "         1.        , 0.99999994],\n",
       "        [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "         1.        , 1.        ],\n",
       "        [1.0000001 , 1.0000001 , 1.        , ..., 0.9999999 ,\n",
       "         1.0000001 , 0.99999994],\n",
       "        ...,\n",
       "        [0.99999994, 1.0000001 , 1.        , ..., 1.        ,\n",
       "         0.99999994, 1.        ],\n",
       "        [1.        , 0.9999998 , 0.99999994, ..., 0.99999994,\n",
       "         1.0000002 , 1.        ],\n",
       "        [1.        , 1.        , 1.0000001 , ..., 1.        ,\n",
       "         0.99999994, 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.0000001 , ..., 1.        ,\n",
       "         1.        , 1.0000001 ],\n",
       "        [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "         0.99999994, 1.        ],\n",
       "        [1.        , 1.        , 0.99999994, ..., 0.99999994,\n",
       "         1.        , 0.99999976],\n",
       "        ...,\n",
       "        [0.9999999 , 1.0000001 , 1.        , ..., 1.        ,\n",
       "         0.99999994, 1.        ],\n",
       "        [0.99999994, 1.        , 1.        , ..., 0.99999976,\n",
       "         1.0000001 , 1.        ],\n",
       "        [0.99999994, 0.99999994, 1.        , ..., 0.9999998 ,\n",
       "         1.        , 0.99999994]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 1.        , 1.        , ..., 1.        ,\n",
       "         1.0000001 , 1.        ],\n",
       "        [1.        , 0.9999999 , 0.9999999 , ..., 1.        ,\n",
       "         1.0000001 , 0.99999994],\n",
       "        [1.        , 1.0000001 , 1.        , ..., 1.        ,\n",
       "         0.99999994, 1.0000002 ],\n",
       "        ...,\n",
       "        [1.        , 0.99999994, 1.        , ..., 1.        ,\n",
       "         0.99999994, 0.9999998 ],\n",
       "        [1.        , 1.        , 1.0000001 , ..., 1.        ,\n",
       "         1.        , 0.9999999 ],\n",
       "        [1.        , 1.        , 0.99999994, ..., 0.9999999 ,\n",
       "         0.99999994, 1.        ]],\n",
       "\n",
       "       [[1.        , 0.99999994, 0.99999994, ..., 1.        ,\n",
       "         1.        , 1.        ],\n",
       "        [0.9999999 , 1.        , 1.        , ..., 0.9999999 ,\n",
       "         1.        , 1.        ],\n",
       "        [1.0000001 , 1.0000001 , 1.0000001 , ..., 1.0000001 ,\n",
       "         1.0000001 , 0.9999999 ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "         1.        , 1.0000001 ],\n",
       "        [1.        , 1.0000001 , 1.0000001 , ..., 1.        ,\n",
       "         0.9999999 , 0.99999994],\n",
       "        [0.9999999 , 1.        , 0.9999999 , ..., 1.0000001 ,\n",
       "         0.9999999 , 1.        ]],\n",
       "\n",
       "       [[0.99999994, 1.        , 1.        , ..., 1.        ,\n",
       "         1.        , 1.        ],\n",
       "        [1.0000001 , 1.        , 1.0000001 , ..., 1.        ,\n",
       "         1.        , 1.0000001 ],\n",
       "        [1.        , 1.        , 1.        , ..., 1.        ,\n",
       "         1.0000001 , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.0000001 , 1.        , ..., 1.        ,\n",
       "         1.        , 1.        ],\n",
       "        [0.9999999 , 0.9999999 , 1.        , ..., 0.9999999 ,\n",
       "         1.        , 0.9999998 ],\n",
       "        [1.        , 0.99999994, 1.        , ..., 0.9999998 ,\n",
       "         1.        , 0.99999994]]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.convert_to_tensor(outputs.attentions[0][:100]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c8cbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 13:10:34.915050: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.87GiB (rounded to 5233680128)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-06-07 13:10:34.915114: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] BFCAllocator dump for GPU_0_bfc\n",
      "2022-06-07 13:10:34.915136: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (256): \tTotal Chunks: 7, Chunks in use: 5. 1.8KiB allocated for chunks. 1.2KiB in use in bin. 56B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915151: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915166: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915180: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915193: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915206: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915219: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915237: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (32768): \tTotal Chunks: 2, Chunks in use: 1. 95.2KiB allocated for chunks. 57.0KiB in use in bin. 56.8KiB client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915250: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915263: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915276: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915289: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915302: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915316: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915329: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915342: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915355: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915368: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915381: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915397: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915411: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 4.00GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-06-07 13:10:34.915427: I tensorflow/core/common_runtime/bfc_allocator.cc:1050] Bin for 4.87GiB was 256.00MiB, Chunk State: \n",
      "2022-06-07 13:10:34.915449: I tensorflow/core/common_runtime/bfc_allocator.cc:1056]   Size: 4.00GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 57.0KiB | Requested Size: 56.8KiB | in_use: 1 | bin_num: -1\n",
      "2022-06-07 13:10:34.915461: I tensorflow/"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cl2022/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/cl2022/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core/common_runtime/bfc_allocator.cc:1063] Next region of size 4294967296\n",
      "2022-06-07 13:10:34.915477: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f8578000000 of size 256 next 1\n",
      "2022-06-07 13:10:34.915491: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f8578000100 of size 1280 next 2\n",
      "2022-06-07 13:10:34.915503: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f8578000600 of size 256 next 3\n",
      "2022-06-07 13:10:34.915513: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f8578000700 of size 256 next 4\n",
      "2022-06-07 13:10:34.915524: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] Free  at 7f8578000800 of size 256 next 5\n",
      "2022-06-07 13:10:34.915535: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f8578000900 of size 256 next 6\n",
      "2022-06-07 13:10:34.915545: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] Free  at 7f8578000a00 of size 39168 next 8\n",
      "2022-06-07 13:10:34.915556: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f857800a300 of size 256 next 9\n",
      "2022-06-07 13:10:34.915567: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] Free  at 7f857800a400 of size 256 next 10\n",
      "2022-06-07 13:10:34.915578: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7f857800a500 of size 58368 next 11\n",
      "2022-06-07 13:10:34.915590: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] Free  at 7f8578018900 of size 4294866688 next 18446744073709551615\n",
      "2022-06-07 13:10:34.915601: I tensorflow/core/common_runtime/bfc_allocator.cc:1088]      Summary of in-use Chunks by size: \n",
      "2022-06-07 13:10:34.915615: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 5 Chunks of size 256 totalling 1.2KiB\n",
      "2022-06-07 13:10:34.915628: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-06-07 13:10:34.915646: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 58368 totalling 57.0KiB\n",
      "2022-06-07 13:10:34.915670: I tensorflow/core/common_runtime/bfc_allocator.cc:1095] Sum Total of in-use chunks: 59.5KiB\n",
      "2022-06-07 13:10:34.915688: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] total_region_allocated_bytes_: 4294967296 memory_limit_: 4294967296 available bytes: 0 curr_region_allocation_bytes_: 8589934592\n",
      "2022-06-07 13:10:34.915727: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] Stats: \n",
      "Limit:                      4294967296\n",
      "InUse:                           60928\n",
      "MaxInUse:                       100608\n",
      "NumAllocs:                          24\n",
      "MaxAllocSize:                    58368\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-06-07 13:10:34.915763: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *___________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.convert_to_tensor(outputs.attentions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c61f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4846, 150, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f681d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08c9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa10a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc8404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf312f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6fe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6094090",
   "metadata": {},
   "source": [
    "## 2-1. FinBERT's Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.math.argmax(tf.convert_to_tensor(outputs.logits), axis=1)\n",
    "Y_true = tf.math.argmax(Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94256941",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = tf.math.reduce_sum(tf.cast((Y_pred == Y_true), tf.float32)) / len(Y_true)\n",
    "print('Number of examples: %d' % len(Y_true))\n",
    "print('Accuracy of FinBERT on Financial PhraseBank: %.4f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983e4fe",
   "metadata": {},
   "source": [
    "## 2-2. Layer-wise Classification Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449017eb",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model = keras.Sequential()\n",
    "probe_model.add(keras.Input(shape=(outputs.hidden_states[-1].shape[-1])))\n",
    "probe_model.add(Dense(3, activation='softmax', name='output_layer'))\n",
    "print((probe_model.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.save_weights('probe_model_init.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483da2b",
   "metadata": {},
   "source": [
    "### Training Layer-wise Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacef9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '20220603_finbert_sentiment_layer_probe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = len(outputs.hidden_states)\n",
    "print('Results:')\n",
    "print('Layer\\t| train_acc\\t| epoch\\t\\t| val_acc\\t| epoch')\n",
    "for i in range(num_layer):\n",
    "    X = tf.convert_to_tensor(outputs.hidden_states[i])[:, 0, :]\n",
    "    probe_model.load_weights('probe_model_init.h5')\n",
    "    \n",
    "    ckp_path = exp_name + 'layer%2d' % i + '/cp-{epoch:04d}.ckpt'\n",
    "    cp_callback = ModelCheckpoint(filepath=ckp_path, verbose=False,\n",
    "                                  save_weights_only=True, save_freq=100)\n",
    "    probe_model.save_weights(ckp_path.format(epoch=0))\n",
    "    history = probe_model.fit(X, Y, epochs=1000, batch_size=batch_size, verbose=False,\n",
    "                              validation_split=0.3, callbacks=[cp_callback])\n",
    "\n",
    "#     train_loss = np.min(np.array(history.history['loss']))\n",
    "#     val_loss = np.min(np.array(history.history['val_loss']))\n",
    "    train_acc = np.max(np.array(history.history['accuracy']))\n",
    "    train_epoch = np.argmax(np.array(history.history['accuracy']))\n",
    "    val_acc = np.max(np.array(history.history['val_accuracy']))\n",
    "    val_epoch = np.argmax(np.array(history.history['val_accuracy']))\n",
    "    print('%d\\t| %.4f\\t| %d\\t\\t| %.4f\\t| %d' % (i, train_acc, train_epoch, val_acc, val_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac3233",
   "metadata": {},
   "source": [
    "## 2-1. Layer-wise Classification - Non-linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model = keras.Sequential()\n",
    "probe_model.add(keras.Input(shape=(outputs.hidden_states[-1].shape[-1])))\n",
    "probe_model.add(Dense(256, activation='gelu', name='dense_layer'))\n",
    "probe_model.add(Dropout(0.1, name='dropout_layer'))\n",
    "probe_model.add(Dense(3, activation='softmax', name='output_layer'))\n",
    "print((probe_model.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ed289",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc233d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.save_weights('probe_model_init.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e50092",
   "metadata": {},
   "source": [
    "### Training Layer-wise Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '20220603_finbert_sentiment_layer_probe_nonlinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = len(outputs.hidden_states)\n",
    "print('Results:')\n",
    "print('Layer\\t| train_acc\\t| epoch\\t\\t| val_acc\\t| epoch')\n",
    "for i in range(num_layer):\n",
    "    X = tf.convert_to_tensor(outputs.hidden_states[i])[:, 0, :]\n",
    "    probe_model.load_weights('probe_model_init.h5')\n",
    "    \n",
    "    ckp_path = exp_name + 'layer%2d' % i + '/cp-{epoch:04d}.ckpt'\n",
    "    cp_callback = ModelCheckpoint(filepath=ckp_path, verbose=False,\n",
    "                                  save_weights_only=True, save_freq=100)\n",
    "    probe_model.save_weights(ckp_path.format(epoch=0))\n",
    "    history = probe_model.fit(X, Y, epochs=1000, batch_size=batch_size, verbose=False,\n",
    "                              validation_split=0.3, callbacks=[cp_callback])\n",
    "\n",
    "#     train_loss = np.min(np.array(history.history['loss']))\n",
    "#     val_loss = np.min(np.array(history.history['val_loss']))\n",
    "    train_acc = np.max(np.array(history.history['accuracy']))\n",
    "    train_epoch = np.argmax(np.array(history.history['accuracy']))\n",
    "    val_acc = np.max(np.array(history.history['val_accuracy']))\n",
    "    val_epoch = np.argmax(np.array(history.history['val_accuracy']))\n",
    "    print('%d\\t| %.4f\\t| %d\\t\\t| %.4f\\t| %d' % (i, train_acc, train_epoch, val_acc, val_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ab786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
