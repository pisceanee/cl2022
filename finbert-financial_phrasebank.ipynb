{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b905323c",
   "metadata": {},
   "source": [
    "# 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c38354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699fa063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0976be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_logical_device_configuration(gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83a6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 11:11:55.644155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-07 11:11:56.355648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:a6:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from pprint import pprint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13729a",
   "metadata": {},
   "source": [
    "# 1. Data and Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f7250",
   "metadata": {},
   "source": [
    "## 1-1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cc263",
   "metadata": {},
   "source": [
    "### FinPhrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e416b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fpath = './data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec79a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total data: 4846\n",
      "\n",
      "Data examples:\n",
      "['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral', 'Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .@neutral']\n"
     ]
    }
   ],
   "source": [
    "with open(data_fpath, 'rb') as file:\n",
    "    data = file.read()\n",
    "data = data.decode('utf-8', 'ignore')\n",
    "data = data.split('\\r\\n')[:-1]\n",
    "print('Number of total data: %d\\n' % len(data))\n",
    "print('Data examples:')\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1f71c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title examples:\n",
      "['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .']\n",
      "\n",
      "Label examples\n",
      "['neutral', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "titles = [line.split('@')[0] for line in data]\n",
    "labels = [line.split('@')[1] for line in data]\n",
    "print('Title examples:')\n",
    "print(titles[:2])\n",
    "print('\\nLabel examples')\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf18c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sech/anaconda3/envs/cl2022/lib/python3.8/site-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "label_list = ['positive', 'negative', 'neutral']\n",
    "Y = tf.keras.layers.StringLookup(vocabulary=label_list,\n",
    "                                 num_oov_indices=0, output_mode='one_hot')(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905f200",
   "metadata": {},
   "source": [
    "### SemEval 2017 Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e5fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fpath = './data/Headline_Trainingdata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcd21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_fpath, 'r', encoding='utf-8') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# print('Number of total data: %d\\n' % len(data))\n",
    "# print('Data examples:')\n",
    "# pprint(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d97e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = []\n",
    "# companies = []\n",
    "# titles = []\n",
    "# sentiments = []\n",
    "# for i in range(len(data)):\n",
    "#     ids.append(data[i]['id'])\n",
    "#     companies.append(data[i]['company'])\n",
    "#     titles.append(data[i]['title'])\n",
    "#     sentiments.append(data[i]['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e587a07",
   "metadata": {},
   "source": [
    "## 1-2. FinBERT Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2241cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ProsusAI/finbert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "config = AutoConfig.from_pretrained('ProsusAI/finbert',\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cefc87",
   "metadata": {},
   "source": [
    "## 1-3. Vectorization to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83fa9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(titles, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9fb82f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 3min 5s, sys: 17min 54s, total: 1h 21min\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a301cb",
   "metadata": {},
   "source": [
    "# 2. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6094090",
   "metadata": {},
   "source": [
    "## 2-1. FinBERT's Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "827b7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.math.argmax(tf.convert_to_tensor(outputs.logits), axis=1)\n",
    "Y_true = tf.math.argmax(Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94256941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 4846\n",
      "Accuracy of FinBERT on Financial PhraseBank: 0.8894\n"
     ]
    }
   ],
   "source": [
    "acc = tf.math.reduce_sum(tf.cast((Y_pred == Y_true), tf.float32)) / len(Y_true)\n",
    "print('Number of examples: %d' % len(Y_true))\n",
    "print('Accuracy of FinBERT on Financial PhraseBank: %.4f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983e4fe",
   "metadata": {},
   "source": [
    "## 2-2. Layer-wise Classification Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449017eb",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d50d73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " output_layer (Dense)        (None, 3)                 2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "probe_model = keras.Sequential()\n",
    "probe_model.add(keras.Input(shape=(outputs.hidden_states[-1].shape[-1])))\n",
    "probe_model.add(Dense(3, activation='softmax', name='output_layer'))\n",
    "print((probe_model.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f9d3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3c25bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.save_weights('probe_model_init.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483da2b",
   "metadata": {},
   "source": [
    "### Training Layer-wise Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cacef9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '20220603_finbert_sentiment_layer_probe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6cd4a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Layer\t| train_acc\t| epoch\t\t| val_acc\t| epoch\n",
      "0\t| 0.5967\t| 2\t\t| 0.5880\t| 1\n",
      "1\t| 0.7099\t| 988\t\t| 0.5887\t| 22\n",
      "2\t| 0.7597\t| 990\t\t| 0.5880\t| 2\n",
      "3\t| 0.7556\t| 996\t\t| 0.5887\t| 23\n",
      "4\t| 0.7771\t| 981\t\t| 0.5880\t| 1\n",
      "5\t| 0.7916\t| 998\t\t| 0.5880\t| 1\n",
      "6\t| 0.8305\t| 998\t\t| 0.5880\t| 1\n",
      "7\t| 0.8491\t| 974\t\t| 0.5825\t| 6\n",
      "8\t| 0.8488\t| 973\t\t| 0.5880\t| 7\n",
      "9\t| 0.8585\t| 975\t\t| 0.6032\t| 998\n",
      "10\t| 0.8906\t| 993\t\t| 0.7772\t| 996\n",
      "11\t| 0.9287\t| 995\t\t| 0.8817\t| 27\n",
      "12\t| 0.9452\t| 997\t\t| 0.9099\t| 22\n"
     ]
    }
   ],
   "source": [
    "num_layer = len(outputs.hidden_states)\n",
    "print('Results:')\n",
    "print('Layer\\t| train_acc\\t| epoch\\t\\t| val_acc\\t| epoch')\n",
    "for i in range(num_layer):\n",
    "    X = tf.convert_to_tensor(outputs.hidden_states[i])[:, 0, :]\n",
    "    probe_model.load_weights('probe_model_init.h5')\n",
    "    \n",
    "    ckp_path = exp_name + 'layer%2d' % i + '/cp-{epoch:04d}.ckpt'\n",
    "    cp_callback = ModelCheckpoint(filepath=ckp_path, verbose=False,\n",
    "                                  save_weights_only=True, save_freq=100)\n",
    "    probe_model.save_weights(ckp_path.format(epoch=0))\n",
    "    history = probe_model.fit(X, Y, epochs=1000, batch_size=batch_size, verbose=False,\n",
    "                              validation_split=0.3, callbacks=[cp_callback])\n",
    "\n",
    "#     train_loss = np.min(np.array(history.history['loss']))\n",
    "#     val_loss = np.min(np.array(history.history['val_loss']))\n",
    "    train_acc = np.max(np.array(history.history['accuracy']))\n",
    "    train_epoch = np.argmax(np.array(history.history['accuracy']))\n",
    "    val_acc = np.max(np.array(history.history['val_accuracy']))\n",
    "    val_epoch = np.argmax(np.array(history.history['val_accuracy']))\n",
    "    print('%d\\t| %.4f\\t| %d\\t\\t| %.4f\\t| %d' % (i, train_acc, train_epoch, val_acc, val_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac3233",
   "metadata": {},
   "source": [
    "## 2-1. Layer-wise Classification - Non-linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c7a326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_layer (Dense)         (None, 256)               196864    \n",
      "                                                                 \n",
      " dropout_layer (Dropout)     (None, 256)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197,635\n",
      "Trainable params: 197,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "probe_model = keras.Sequential()\n",
    "probe_model.add(keras.Input(shape=(outputs.hidden_states[-1].shape[-1])))\n",
    "probe_model.add(Dense(256, activation='gelu', name='dense_layer'))\n",
    "probe_model.add(Dropout(0.1, name='dropout_layer'))\n",
    "probe_model.add(Dense(3, activation='softmax', name='output_layer'))\n",
    "print((probe_model.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "996ed289",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc233d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_model.save_weights('probe_model_init.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e50092",
   "metadata": {},
   "source": [
    "### Training Layer-wise Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f05e3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = '20220603_finbert_sentiment_layer_probe_nonlinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4addc251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Layer\t| train_acc\t| epoch\t\t| val_acc\t| epoch\n",
      "0\t| 0.5976\t| 73\t\t| 0.5880\t| 0\n",
      "1\t| 0.7703\t| 996\t\t| 0.5880\t| 0\n",
      "2\t| 0.8744\t| 998\t\t| 0.5880\t| 0\n",
      "3\t| 0.8417\t| 991\t\t| 0.5894\t| 23\n",
      "4\t| 0.8930\t| 997\t\t| 0.5887\t| 25\n",
      "5\t| 0.9458\t| 997\t\t| 0.5880\t| 0\n",
      "6\t| 0.9782\t| 977\t\t| 0.6066\t| 730\n",
      "7\t| 0.9976\t| 932\t\t| 0.6279\t| 270\n",
      "8\t| 0.9947\t| 987\t\t| 0.6369\t| 492\n",
      "9\t| 0.9985\t| 967\t\t| 0.6726\t| 376\n",
      "10\t| 0.9994\t| 858\t\t| 0.7978\t| 250\n",
      "11\t| 0.9997\t| 641\t\t| 0.8803\t| 12\n",
      "12\t| 0.9997\t| 691\t\t| 0.9106\t| 3\n"
     ]
    }
   ],
   "source": [
    "num_layer = len(outputs.hidden_states)\n",
    "print('Results:')\n",
    "print('Layer\\t| train_acc\\t| epoch\\t\\t| val_acc\\t| epoch')\n",
    "for i in range(num_layer):\n",
    "    X = tf.convert_to_tensor(outputs.hidden_states[i])[:, 0, :]\n",
    "    probe_model.load_weights('probe_model_init.h5')\n",
    "    \n",
    "    ckp_path = exp_name + 'layer%2d' % i + '/cp-{epoch:04d}.ckpt'\n",
    "    cp_callback = ModelCheckpoint(filepath=ckp_path, verbose=False,\n",
    "                                  save_weights_only=True, save_freq=100)\n",
    "    probe_model.save_weights(ckp_path.format(epoch=0))\n",
    "    history = probe_model.fit(X, Y, epochs=1000, batch_size=batch_size, verbose=False,\n",
    "                              validation_split=0.3, callbacks=[cp_callback])\n",
    "\n",
    "#     train_loss = np.min(np.array(history.history['loss']))\n",
    "#     val_loss = np.min(np.array(history.history['val_loss']))\n",
    "    train_acc = np.max(np.array(history.history['accuracy']))\n",
    "    train_epoch = np.argmax(np.array(history.history['accuracy']))\n",
    "    val_acc = np.max(np.array(history.history['val_accuracy']))\n",
    "    val_epoch = np.argmax(np.array(history.history['val_accuracy']))\n",
    "    print('%d\\t| %.4f\\t| %d\\t\\t| %.4f\\t| %d' % (i, train_acc, train_epoch, val_acc, val_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ab786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
